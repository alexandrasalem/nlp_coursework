{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch - A Quick Walkthrough\n",
    "\n",
    "[PyTorch](https://pytorch.org/) is a Python library for numerical computing and neural networks. Notably, it supports the automatic differentiation behavior that we discussed in class. What does this mean? It means that you can set up arbitrarily complex computational graphs, and PyTorch will take care of differentiating their contents.\n",
    "\n",
    "## 1. Installation!\n",
    "\n",
    "Installation is simple, and can be done using either Pip or Anaconda. The PyTorch homepage has a [helpful installation](https://pytorch.org/#pip-install-pytorch) tool that will help you figure out what the right way will be to install it on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (1.0.0)\n",
      "\u001b[33mYou are using pip version 18.0, however version 19.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've got PyTorch installed, import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. First steps\n",
    "\n",
    "They key data structure in PyTorch is the [`Tensor`](https://pytorch.org/docs/stable/tensors.html), which represents a tensor (loosely, a matrix of arbitrary dimension) that keeps track of its own gradient and can be part of a computational graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5004, 0.8656, 0.1164, 0.6271])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = torch.rand(4)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see that we have created a tensor, populated with 4 random numbers. Each tensor has a `size` attribute, indicating its dimensionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tensor has one dimension, and has 4 elements in that single dimension. We can create a multidimensional random tensor like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1954, 0.9334, 0.1167, 0.2486],\n",
       "        [0.5250, 0.5486, 0.6245, 0.0106]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2 = torch.rand(2, 4) # make a random 2x4 matrix\n",
    "r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tensor has _two_ dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to create a tensor directly from already-existing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_data = [1,2,3,4]\n",
    "some_tensor = torch.tensor(some_data)\n",
    "some_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1,2,3]]).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works with numpy data, as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 1, 0, 2, 2, 2, 0, 0, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "numpy_random = np.random.randint(4, size=10)\n",
    "numpy_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 2, 1, 0, 2, 2, 2, 0, 0, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(numpy_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 2, 1, 0, 2, 2, 2, 0, 0, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(numpy_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can go _back_ into numpy, if for some reason you need that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_tensor.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostly, this is un-necessary, as PyTorch tensors support much of the same functionality as Numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_tensor.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_tensor.sum().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_tensor.sum().unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[[[3]]]]).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can have data types, specified very similarly to how it is done in Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_tensor = torch.tensor([1,2,3], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.], dtype=torch.float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_tensor.double()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some functionality is only implemented on certain types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.6931, 1.0986])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_tensor.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\"log\" not implemented for 'torch.LongTensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-1db4d96ac980>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msome_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# this won't work\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: \"log\" not implemented for 'torch.LongTensor'"
     ]
    }
   ],
   "source": [
    "some_tensor.log() # this won't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.6931, 1.0986, 1.3863])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_tensor.float().log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can, of course, do arithemtic with tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5000, 2.0500, 3.6000])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1.0, 2.3, 4.6])\n",
    "b = torch.tensor([0.5, 0.25, 1.0])\n",
    "a-b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notably, if you are using a GPU, tensors can be transparently used on either the CPU or the GPU; memory allocation, etc. will all be done automatically, and if the arithmetic operations you are requesting are available in CUDA, the appropriate kernel will be run for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy-style array sicing works, and in multiple dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1597, 0.6460, 0.8195, 0.2139, 0.8933, 0.0634],\n",
       "        [0.2876, 0.7677, 0.6954, 0.3711, 0.4428, 0.4030],\n",
       "        [0.6695, 0.3296, 0.1785, 0.9174, 0.7979, 0.1513],\n",
       "        [0.6871, 0.3646, 0.7385, 0.4903, 0.7017, 0.3219]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r3 = torch.rand(4,6)\n",
    "r3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2876, 0.7677, 0.6954, 0.3711, 0.4428, 0.4030])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r3[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8195, 0.6954, 0.1785, 0.7385])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r3[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6871, 0.3646, 0.7385, 0.4903, 0.7017, 0.3219])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r3[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix and vector operations are, of course supported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  4,  9, 16])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3,4])\n",
    "b = torch.tensor([1,2,3,4])\n",
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5, 12, 21, 32]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.tensor([[5,6,7,8]]) # a 1x4 matrix\n",
    "c * a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `unsqueeze()` function to turn vectors into matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_mat = a.unsqueeze(0)\n",
    "a_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_mat.shape: torch.Size([1, 4])\n",
      "c.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "print(f\"a_mat.shape: {a_mat.shape}\")\n",
    "print(f\"c.shape: {c.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5,  6,  7,  8],\n",
       "        [10, 12, 14, 16],\n",
       "        [15, 18, 21, 24],\n",
       "        [20, 24, 28, 32]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_mat.t() @ c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't want to or can't use Python 3's matrix multiplication operator, there is a `matmul` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5,  6,  7,  8],\n",
       "        [10, 12, 14, 16],\n",
       "        [15, 18, 21, 24],\n",
       "        [20, 24, 28, 32]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_mat.t().matmul(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variety of \"reduction\" operators are included, as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting things together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(30)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_dot_b = (a * b).sum()\n",
    "a_dot_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, dot (inner) product is also included:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(30)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dot(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradients\n",
    "\n",
    "One notable thing about PyTorch tensors (and PyTorch's various arithmetic operators!) is that they keep track of their gradients. This is, of course, important for neural network training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = torch.tensor(2.0, requires_grad=True)\n",
    "z = torch.tensor(0.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = x + y\n",
    "b = torch.max(y, z)\n",
    "f = a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.retain_grad() # tell PyTorch to retain gradients from f's computational history\n",
    "f.backward() # calculate gradients relative to f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(f.grad) # df/df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad # df/dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad # df/dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.grad # df/dz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing you'll note is that PyTorch goes out of its way to be efficient- for a large network, keeping around all gradients would quickly become prohibitive in terms of memory usage, so by default it only keeps around the bare minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimization\n",
    "\n",
    "Let's try implementing a multinomial logistic regression classifier. Recall that logistic regression looks like so:\n",
    "\n",
    "$$\\hat{y} = \\frac{e^{x*w_c^T+b}}{\\sum^{C}_{j=1}e^{x*w_j^T+b}}$$\n",
    "\n",
    "This is equivalent to:\n",
    "\n",
    "$$\\hat{y}=\\log \\text{softmax}(w^T*x+b)$$\n",
    "\n",
    "We need to learn the parameters of the matrix $w$ and bias term $b$, and do so by minimizing the negativel log-likelihood of $\\hat{y}$.\n",
    "\n",
    "PyTorch comes with implementations of several optimization algorithms that we can use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "iris_x = iris['data']\n",
    "iris_y = iris['target']\n",
    "iris_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]), array([50, 50, 50]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(iris_y, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up a 75/25 train/test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = np.arange(iris_x.shape[0]) % 4 != 0 # get an array where every fourth item is marked for inclusion in test\n",
    "iris_x_train_np, iris_y_train_np = iris_x[train_idx, :], iris_y[train_idx]\n",
    "iris_x_test_np, iris_y_test_np = iris_x[~train_idx, :], iris_y[~train_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can turn these straight into PyTorch tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_x_train, iris_y_train = torch.from_numpy(iris_x_train_np), torch.from_numpy(iris_y_train_np)\n",
    "iris_x_test, iris_y_test = torch.from_numpy(iris_x_test_np), torch.from_numpy(iris_y_test_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `from_numpy()` by default matches `dtype`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_x_test_np.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_x_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.9000, 3.0000, 1.4000, 0.2000],\n",
       "        [4.7000, 3.2000, 1.3000, 0.2000],\n",
       "        [4.6000, 3.1000, 1.5000, 0.2000],\n",
       "        [5.4000, 3.9000, 1.7000, 0.4000],\n",
       "        [4.6000, 3.4000, 1.4000, 0.3000],\n",
       "        [5.0000, 3.4000, 1.5000, 0.2000],\n",
       "        [4.9000, 3.1000, 1.5000, 0.1000],\n",
       "        [5.4000, 3.7000, 1.5000, 0.2000],\n",
       "        [4.8000, 3.4000, 1.6000, 0.2000],\n",
       "        [4.3000, 3.0000, 1.1000, 0.1000],\n",
       "        [5.8000, 4.0000, 1.2000, 0.2000],\n",
       "        [5.7000, 4.4000, 1.5000, 0.4000],\n",
       "        [5.1000, 3.5000, 1.4000, 0.3000],\n",
       "        [5.7000, 3.8000, 1.7000, 0.3000],\n",
       "        [5.1000, 3.8000, 1.5000, 0.3000],\n",
       "        [5.1000, 3.7000, 1.5000, 0.4000],\n",
       "        [4.6000, 3.6000, 1.0000, 0.2000],\n",
       "        [5.1000, 3.3000, 1.7000, 0.5000],\n",
       "        [5.0000, 3.0000, 1.6000, 0.2000],\n",
       "        [5.0000, 3.4000, 1.6000, 0.4000],\n",
       "        [5.2000, 3.5000, 1.5000, 0.2000],\n",
       "        [4.7000, 3.2000, 1.6000, 0.2000],\n",
       "        [4.8000, 3.1000, 1.6000, 0.2000],\n",
       "        [5.4000, 3.4000, 1.5000, 0.4000],\n",
       "        [5.5000, 4.2000, 1.4000, 0.2000],\n",
       "        [4.9000, 3.1000, 1.5000, 0.2000],\n",
       "        [5.0000, 3.2000, 1.2000, 0.2000],\n",
       "        [4.9000, 3.6000, 1.4000, 0.1000],\n",
       "        [4.4000, 3.0000, 1.3000, 0.2000],\n",
       "        [5.1000, 3.4000, 1.5000, 0.2000],\n",
       "        [4.5000, 2.3000, 1.3000, 0.3000],\n",
       "        [4.4000, 3.2000, 1.3000, 0.2000],\n",
       "        [5.0000, 3.5000, 1.6000, 0.6000],\n",
       "        [4.8000, 3.0000, 1.4000, 0.3000],\n",
       "        [5.1000, 3.8000, 1.6000, 0.2000],\n",
       "        [4.6000, 3.2000, 1.4000, 0.2000],\n",
       "        [5.0000, 3.3000, 1.4000, 0.2000],\n",
       "        [7.0000, 3.2000, 4.7000, 1.4000],\n",
       "        [6.4000, 3.2000, 4.5000, 1.5000],\n",
       "        [5.5000, 2.3000, 4.0000, 1.3000],\n",
       "        [6.5000, 2.8000, 4.6000, 1.5000],\n",
       "        [5.7000, 2.8000, 4.5000, 1.3000],\n",
       "        [4.9000, 2.4000, 3.3000, 1.0000],\n",
       "        [6.6000, 2.9000, 4.6000, 1.3000],\n",
       "        [5.2000, 2.7000, 3.9000, 1.4000],\n",
       "        [5.9000, 3.0000, 4.2000, 1.5000],\n",
       "        [6.0000, 2.2000, 4.0000, 1.0000],\n",
       "        [6.1000, 2.9000, 4.7000, 1.4000],\n",
       "        [6.7000, 3.1000, 4.4000, 1.4000],\n",
       "        [5.6000, 3.0000, 4.5000, 1.5000],\n",
       "        [5.8000, 2.7000, 4.1000, 1.0000],\n",
       "        [5.6000, 2.5000, 3.9000, 1.1000],\n",
       "        [5.9000, 3.2000, 4.8000, 1.8000],\n",
       "        [6.1000, 2.8000, 4.0000, 1.3000],\n",
       "        [6.1000, 2.8000, 4.7000, 1.2000],\n",
       "        [6.4000, 2.9000, 4.3000, 1.3000],\n",
       "        [6.6000, 3.0000, 4.4000, 1.4000],\n",
       "        [6.7000, 3.0000, 5.0000, 1.7000],\n",
       "        [6.0000, 2.9000, 4.5000, 1.5000],\n",
       "        [5.7000, 2.6000, 3.5000, 1.0000],\n",
       "        [5.5000, 2.4000, 3.7000, 1.0000],\n",
       "        [5.8000, 2.7000, 3.9000, 1.2000],\n",
       "        [6.0000, 2.7000, 5.1000, 1.6000],\n",
       "        [6.0000, 3.4000, 4.5000, 1.6000],\n",
       "        [6.7000, 3.1000, 4.7000, 1.5000],\n",
       "        [6.3000, 2.3000, 4.4000, 1.3000],\n",
       "        [5.5000, 2.5000, 4.0000, 1.3000],\n",
       "        [5.5000, 2.6000, 4.4000, 1.2000],\n",
       "        [6.1000, 3.0000, 4.6000, 1.4000],\n",
       "        [5.0000, 2.3000, 3.3000, 1.0000],\n",
       "        [5.6000, 2.7000, 4.2000, 1.3000],\n",
       "        [5.7000, 3.0000, 4.2000, 1.2000],\n",
       "        [6.2000, 2.9000, 4.3000, 1.3000],\n",
       "        [5.1000, 2.5000, 3.0000, 1.1000],\n",
       "        [5.7000, 2.8000, 4.1000, 1.3000],\n",
       "        [5.8000, 2.7000, 5.1000, 1.9000],\n",
       "        [7.1000, 3.0000, 5.9000, 2.1000],\n",
       "        [6.3000, 2.9000, 5.6000, 1.8000],\n",
       "        [7.6000, 3.0000, 6.6000, 2.1000],\n",
       "        [4.9000, 2.5000, 4.5000, 1.7000],\n",
       "        [7.3000, 2.9000, 6.3000, 1.8000],\n",
       "        [7.2000, 3.6000, 6.1000, 2.5000],\n",
       "        [6.5000, 3.2000, 5.1000, 2.0000],\n",
       "        [6.4000, 2.7000, 5.3000, 1.9000],\n",
       "        [5.7000, 2.5000, 5.0000, 2.0000],\n",
       "        [5.8000, 2.8000, 5.1000, 2.4000],\n",
       "        [6.4000, 3.2000, 5.3000, 2.3000],\n",
       "        [7.7000, 3.8000, 6.7000, 2.2000],\n",
       "        [7.7000, 2.6000, 6.9000, 2.3000],\n",
       "        [6.0000, 2.2000, 5.0000, 1.5000],\n",
       "        [5.6000, 2.8000, 4.9000, 2.0000],\n",
       "        [7.7000, 2.8000, 6.7000, 2.0000],\n",
       "        [6.3000, 2.7000, 4.9000, 1.8000],\n",
       "        [7.2000, 3.2000, 6.0000, 1.8000],\n",
       "        [6.2000, 2.8000, 4.8000, 1.8000],\n",
       "        [6.1000, 3.0000, 4.9000, 1.8000],\n",
       "        [7.2000, 3.0000, 5.8000, 1.6000],\n",
       "        [7.4000, 2.8000, 6.1000, 1.9000],\n",
       "        [7.9000, 3.8000, 6.4000, 2.0000],\n",
       "        [6.3000, 2.8000, 5.1000, 1.5000],\n",
       "        [6.1000, 2.6000, 5.6000, 1.4000],\n",
       "        [7.7000, 3.0000, 6.1000, 2.3000],\n",
       "        [6.4000, 3.1000, 5.5000, 1.8000],\n",
       "        [6.0000, 3.0000, 4.8000, 1.8000],\n",
       "        [6.9000, 3.1000, 5.4000, 2.1000],\n",
       "        [6.9000, 3.1000, 5.1000, 2.3000],\n",
       "        [5.8000, 2.7000, 5.1000, 1.9000],\n",
       "        [6.8000, 3.2000, 5.9000, 2.3000],\n",
       "        [6.7000, 3.0000, 5.2000, 2.3000],\n",
       "        [6.3000, 2.5000, 5.0000, 1.9000],\n",
       "        [6.5000, 3.0000, 5.2000, 2.0000],\n",
       "        [5.9000, 3.0000, 5.1000, 1.8000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can set up our parameters, starting with a random initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.rand(3,4, dtype=torch.float64)\n",
    "b = torch.rand(3, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's set up a loss function, as well as import a utility function package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3323, 0.4908, 0.2774, 0.8495],\n",
       "        [0.2168, 0.0769, 0.5469, 0.2591],\n",
       "        [0.7348, 0.0870, 0.2629, 0.6348]], dtype=torch.float64)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3428, 0.2846, 0.4459], dtype=torch.float64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.NLLLoss() # negative log likelihood loss- PyTorch comes with many others out of the box\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall procedure will be:\n",
    "\n",
    "1. Make a prediction\n",
    "2. Compute loss\n",
    "3. Update\n",
    "4. Repeat\n",
    "\n",
    "Let's see what that looks like for one run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = iris_x_train @ w.t() + b\n",
    "y_hat = F.log_softmax(z, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that this worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, dtype=torch.float64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat[0].exp().sum() # should be 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1438, -2.0703, -4.8568],\n",
       "        [-0.1026, -2.3849, -5.2270],\n",
       "        [-0.1473, -2.0608, -4.6433],\n",
       "        [-0.0767, -2.6517, -5.7194],\n",
       "        [-0.0962, -2.4514, -5.1864],\n",
       "        [-0.1016, -2.3896, -5.3083],\n",
       "        [-0.1386, -2.1040, -4.8991],\n",
       "        [-0.0700, -2.7324, -5.9875],\n",
       "        [-0.1161, -2.2745, -5.0027],\n",
       "        [-0.0999, -2.4143, -5.1773],\n",
       "        [-0.0334, -3.4380, -7.2778],\n",
       "        [-0.0327, -3.4641, -7.0604],\n",
       "        [-0.0831, -2.5760, -5.6205],\n",
       "        [-0.0813, -2.5880, -5.8282],\n",
       "        [-0.0659, -2.7967, -5.8793],\n",
       "        [-0.0774, -2.6467, -5.6378],\n",
       "        [-0.0446, -3.1717, -6.3696],\n",
       "        [-0.1630, -1.9653, -4.5785],\n",
       "        [-0.1809, -1.8657, -4.5405],\n",
       "        [-0.1246, -2.2074, -4.9410],\n",
       "        [-0.0894, -2.5046, -5.5713],\n",
       "        [-0.1471, -2.0617, -4.6554],\n",
       "        [-0.1637, -1.9617, -4.5690],\n",
       "        [-0.1079, -2.3273, -5.3591],\n",
       "        [-0.0341, -3.4234, -7.0165],\n",
       "        [-0.1445, -2.0685, -4.8125],\n",
       "        [-0.0893, -2.5047, -5.5913],\n",
       "        [-0.0687, -2.7573, -5.8368],\n",
       "        [-0.1318, -2.1633, -4.7633],\n",
       "        [-0.1010, -2.3925, -5.3651],\n",
       "        [-0.2977, -1.4504, -3.7711],\n",
       "        [-0.1045, -2.3763, -5.0569],\n",
       "        [-0.1207, -2.2410, -4.9126],\n",
       "        [-0.1508, -2.0325, -4.7139],\n",
       "        [-0.0714, -2.7209, -5.7719],\n",
       "        [-0.1164, -2.2727, -4.9782],\n",
       "        [-0.1012, -2.3916, -5.3528],\n",
       "        [-2.7833, -0.4833, -1.1349],\n",
       "        [-2.6839, -0.5668, -1.0096],\n",
       "        [-3.2144, -0.6798, -0.7917],\n",
       "        [-3.3281, -0.6113, -0.8640],\n",
       "        [-3.2682, -0.7285, -0.7355],\n",
       "        [-2.0435, -0.5898, -1.1520],\n",
       "        [-3.0525, -0.5419, -0.9912],\n",
       "        [-2.6786, -0.6953, -0.8384],\n",
       "        [-2.6199, -0.6125, -0.9540],\n",
       "        [-3.0386, -0.5253, -1.0197],\n",
       "        [-3.3965, -0.7045, -0.7505],\n",
       "        [-2.5482, -0.4853, -1.1834],\n",
       "        [-3.1571, -0.7715, -0.7029],\n",
       "        [-2.5920, -0.5499, -1.0552],\n",
       "        [-2.6588, -0.5731, -1.0046],\n",
       "        [-3.4718, -0.8454, -0.6170],\n",
       "        [-2.4297, -0.5169, -1.1534],\n",
       "        [-3.4039, -0.6744, -0.7824],\n",
       "        [-2.6632, -0.5133, -1.1033],\n",
       "        [-2.6927, -0.5062, -1.1101],\n",
       "        [-3.7555, -0.7131, -0.7206],\n",
       "        [-3.1756, -0.6839, -0.7905],\n",
       "        [-1.9267, -0.4962, -1.4042],\n",
       "        [-2.4656, -0.5428, -1.0969],\n",
       "        [-2.4214, -0.5417, -1.1103],\n",
       "        [-4.5076, -0.9989, -0.4769],\n",
       "        [-2.5837, -0.6514, -0.9083],\n",
       "        [-3.0348, -0.5629, -0.9614],\n",
       "        [-3.6170, -0.6202, -0.8317],\n",
       "        [-2.9497, -0.6553, -0.8478],\n",
       "        [-3.3750, -0.7519, -0.7046],\n",
       "        [-3.1072, -0.6579, -0.8270],\n",
       "        [-2.1414, -0.5711, -1.1469],\n",
       "        [-2.9644, -0.6687, -0.8300],\n",
       "        [-2.4947, -0.6043, -0.9915],\n",
       "        [-2.7068, -0.5499, -1.0321],\n",
       "        [-1.5719, -0.5718, -1.4792],\n",
       "        [-2.6602, -0.6108, -0.9490],\n",
       "        [-4.8074, -1.1678, -0.3845],\n",
       "        [-5.4224, -1.1257, -0.3987],\n",
       "        [-5.1277, -1.1744, -0.3782],\n",
       "        [-6.4793, -1.3422, -0.3049],\n",
       "        [-4.2444, -1.1509, -0.4015],\n",
       "        [-5.9756, -1.1995, -0.3622],\n",
       "        [-5.1607, -1.1780, -0.3763],\n",
       "        [-3.9053, -0.8496, -0.5937],\n",
       "        [-4.9251, -1.0614, -0.4358],\n",
       "        [-5.0399, -1.2377, -0.3517],\n",
       "        [-5.0449, -1.3188, -0.3201],\n",
       "        [-4.4957, -1.0675, -0.4386],\n",
       "        [-5.5081, -1.1718, -0.3767],\n",
       "        [-7.7625, -1.6980, -0.2027],\n",
       "        [-4.9831, -1.0376, -0.4481],\n",
       "        [-4.4618, -1.1391, -0.4029],\n",
       "        [-6.8403, -1.3852, -0.2895],\n",
       "        [-4.2034, -0.8678, -0.5706],\n",
       "        [-5.0457, -0.9942, -0.4724],\n",
       "        [-3.9299, -0.8334, -0.6055],\n",
       "        [-3.8489, -0.8667, -0.5827],\n",
       "        [-4.8462, -0.8822, -0.5477],\n",
       "        [-5.8014, -1.1124, -0.4032],\n",
       "        [-4.7631, -0.8841, -0.5475],\n",
       "        [-4.1886, -0.8519, -0.5830],\n",
       "        [-5.3453, -1.1898, -0.3697],\n",
       "        [-5.6912, -1.0878, -0.4160],\n",
       "        [-4.6208, -1.0338, -0.4549],\n",
       "        [-3.7166, -0.8534, -0.5983],\n",
       "        [-4.4834, -0.9127, -0.5323],\n",
       "        [-4.1192, -0.8335, -0.5992],\n",
       "        [-4.8074, -1.1678, -0.3845],\n",
       "        [-5.4026, -1.2530, -0.3427],\n",
       "        [-4.4991, -0.9637, -0.4986],\n",
       "        [-4.7300, -0.9903, -0.4785],\n",
       "        [-4.3560, -0.9375, -0.5182],\n",
       "        [-4.2588, -1.0244, -0.4671]], dtype=torch.float64,\n",
       "       grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_y_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, compute loss.  The loss functions in PyTorch are a bit finicky about their dimensions- the key is that the prediction must be $N \\times C$, and the target must be $N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2320, dtype=torch.float64)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(y_hat[0].unsqueeze(0), iris_y_train[0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we're not pulling out individual items, it's all a lot easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1411, dtype=torch.float64)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(y_hat[:4], iris_y_train[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can compute it on an entire training set at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4790, dtype=torch.float64)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(y_hat, iris_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at that loss function for the subset of the data that was correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1327, dtype=torch.float64)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func( y_hat[:37], iris_y_train[:37])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got our loss calculation, but it would be helpful to have a way to evaluate our parameter's accuracy as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_acc(y_hat_, y_):\n",
    "    n_correct = (y_hat_.argmax(dim=1) == y_).sum().float()\n",
    "    n_total = float(len(y_))\n",
    "    return (n_correct / n_total).item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3303571343421936"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_acc(y_hat, iris_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes sense, since we are working with random weights- we'd expect to get chance accuracy. Let's try optimizing our parameters!\n",
    "\n",
    "Now, we set up an optimizer and training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, training loss: 2.4355409546889577, train acc: 0.3392857015132904\n",
      "epoch 100, training loss: 0.76661684472197, train acc: 0.8035714030265808\n",
      "epoch 200, training loss: 0.5982738852791666, train acc: 0.8482142686843872\n",
      "epoch 300, training loss: 0.5238055790659826, train acc: 0.8928571343421936\n",
      "epoch 400, training loss: 0.47975299213713335, train acc: 0.9017857313156128\n",
      "epoch 500, training loss: 0.4489650463638898, train acc: 0.9285714030265808\n",
      "epoch 600, training loss: 0.425228178926021, train acc: 0.9464285969734192\n",
      "epoch 700, training loss: 0.40578188402154297, train acc: 0.9553571343421936\n",
      "epoch 800, training loss: 0.38921296633362507, train acc: 0.9553571343421936\n",
      "epoch 900, training loss: 0.3747185777873843, train acc: 0.9732142686843872\n"
     ]
    }
   ],
   "source": [
    "# re-initialize our parameters, this time making sure that they keep track of their gradients:\n",
    "\n",
    "w = torch.rand(3,4, dtype=torch.float64, requires_grad=True)\n",
    "b = torch.rand(3, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "sgd_opt = torch.optim.SGD([w, b], lr=0.01) # can experiment with different learning rates\n",
    "\n",
    "num_epochs = 1000\n",
    "\n",
    "stored_perf = [] # for graphing our progress\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    sgd_opt.zero_grad() # erase any gradients our optimizer might remember\n",
    "    \n",
    "    # do a forward pass through our data:\n",
    "    z = iris_x_train @ w.t() + b\n",
    "    y_hat = F.log_softmax(z, dim=1)\n",
    "    \n",
    "    loss = loss_func(y_hat, iris_y_train)\n",
    "    \n",
    "    # send back gradients:\n",
    "    loss.backward()\n",
    "    \n",
    "    # now, tell the optimizer to update our weights:\n",
    "    sgd_opt.step()\n",
    "\n",
    "    epoch_acc = eval_acc(y_hat, iris_y_train)\n",
    "    \n",
    "    # also compute loss on our test data\n",
    "    y_hat_test = F.log_softmax(iris_x_test @ w.t() + b, dim=1)\n",
    "    loss_test = loss_func(y_hat_test, iris_y_test)\n",
    "    acc_test = eval_acc(y_hat_test, iris_y_test)\n",
    "    \n",
    "    stored_perf.append([i, loss.item(), epoch_acc, loss_test.item(), acc_test])\n",
    "    \n",
    "    # print out how we're doing:\n",
    "    if i % 100 == 0:\n",
    "        print(f\"epoch {i}, training loss: {loss}, train acc: {eval_acc(y_hat, iris_y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a few plots, just for fun:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pylab import plt\n",
    "\n",
    "perf_df = pd.DataFrame(stored_perf, columns=[\"epoch\",\"train loss\",\"train acc\", \"test loss\", \"test acc\"])\n",
    "_, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "perf_df[[\"train loss\", \"test loss\"]].plot(ax=ax1);\n",
    "perf_df[[\"train acc\", \"test acc\"]].plot(ax=ax2);\n",
    "ax1.set_ylim([0,6]);\n",
    "ax2.set_ylim([0,1]);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch has built-in classes to do a lot of this work for us. For very simple models like the one above, it doesn't make a huge difference, but as models grow in complexity, it can be extremely helpful!\n",
    "\n",
    "For example, instead of manually performing the matrix multiplication and bias addition, we can use the `linear()` function from `torch.nn.functional`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = F.linear(iris_x_train, w, bias=b)\n",
    "y_hat = F.log_softmax(z, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option is to use the `nn` package's classes to wrap our model functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since our data is coming from Numpy, and it defaults to doubles, we need to tell our model to match:\n",
    "linear_layer = nn.Linear(4,3).double() # input features, output features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, training loss: 1.97799011248781, train acc: 0.3303571343421936\n",
      "epoch 100, training loss: 0.6658864987872504, train acc: 0.8035714030265808\n",
      "epoch 200, training loss: 0.5623656898579767, train acc: 0.8482142686843872\n",
      "epoch 300, training loss: 0.5070933545641688, train acc: 0.9017857313156128\n",
      "epoch 400, training loss: 0.47061457750143226, train acc: 0.9107142686843872\n",
      "epoch 500, training loss: 0.4434213404065468, train acc: 0.9553571343421936\n",
      "epoch 600, training loss: 0.4216054005825045, train acc: 0.9642857313156128\n",
      "epoch 700, training loss: 0.40327153717665226, train acc: 0.9642857313156128\n",
      "epoch 800, training loss: 0.3873850923950462, train acc: 0.9642857313156128\n",
      "epoch 900, training loss: 0.37332812314944136, train acc: 0.9732142686843872\n"
     ]
    }
   ],
   "source": [
    "sgd_opt = torch.optim.SGD(linear_layer.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 1000\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    sgd_opt.zero_grad() # erase any gradients our optimizer might remember\n",
    "    \n",
    "    # do a forward pass through our data:\n",
    "    z = linear_layer(iris_x_train)\n",
    "    y_hat = F.log_softmax(z, dim=1)\n",
    "    \n",
    "    loss = loss_func(y_hat, iris_y_train)\n",
    "    \n",
    "    # send back gradients:\n",
    "    loss.backward()\n",
    "    \n",
    "    # now, tell the optimizer to update our weights:\n",
    "    sgd_opt.step()\n",
    "\n",
    "    epoch_acc = eval_acc(y_hat, iris_y_train)\n",
    "    \n",
    "    # print out how we're doing:\n",
    "    if i % 100 == 0:\n",
    "        print(f\"epoch {i}, training loss: {loss}, train acc: {eval_acc(y_hat, iris_y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had more going on our model, this would ake for cleaner code. We can get even more modular, but using the `nn.Module` class to group layers and functions. Note that our `Module` is just a Python class, so we can e.g. specify parameters for network dimensionality, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg(nn.Module):\n",
    "    def __init__(self, input_dims, output_classes):\n",
    "        super(LogReg, self).__init__()\n",
    "        self.linear = nn.Linear(input_dims, output_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.linear(x)\n",
    "        return F.log_softmax(z, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use our `LogReg` class just like we did when we were just using `Linear` directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.3116, -0.1745,  0.4227, -0.3844],\n",
       "         [ 0.4475, -0.4064, -0.2678, -0.1543],\n",
       "         [-0.0024, -0.3775,  0.4051, -0.3182]], dtype=torch.float64,\n",
       "        requires_grad=True), Parameter containing:\n",
       " tensor([-0.2197, -0.0317, -0.1947], dtype=torch.float64, requires_grad=True)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg = LogReg(4,3).double()\n",
    "\n",
    "list(log_reg.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, training loss: 3.7351197690225066, train acc: 0.3303571343421936\n",
      "epoch 100, training loss: 0.7173455341772231, train acc: 0.6964285969734192\n",
      "epoch 200, training loss: 0.5933143554042346, train acc: 0.7946428656578064\n",
      "epoch 300, training loss: 0.5313941587394178, train acc: 0.8571428656578064\n",
      "epoch 400, training loss: 0.4916220408056237, train acc: 0.8928571343421936\n",
      "epoch 500, training loss: 0.4622516501145401, train acc: 0.9196428656578064\n",
      "epoch 600, training loss: 0.43874522578161923, train acc: 0.9196428656578064\n",
      "epoch 700, training loss: 0.41898942335373845, train acc: 0.9464285969734192\n",
      "epoch 800, training loss: 0.4018601388459532, train acc: 0.9553571343421936\n",
      "epoch 900, training loss: 0.38669648900825, train acc: 0.9642857313156128\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogReg(4,3).double()\n",
    "\n",
    "sgd_opt = torch.optim.SGD(log_reg.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 1000\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    sgd_opt.zero_grad() # erase any gradients our optimizer might remember\n",
    "    \n",
    "    # do a forward pass through our data:\n",
    "    y_hat = log_reg(iris_x_train) # note that we don't need to call softmax here anymore!\n",
    "    #print(y_hat.size())\n",
    "    #print(iris_y_train.size())\n",
    "    \n",
    "    loss = loss_func(y_hat, iris_y_train)\n",
    "    #print(y_hat)\n",
    "    #print(iris_y_train)\n",
    "    #print(loss)\n",
    "    \n",
    "    # send back gradients:\n",
    "    loss.backward()\n",
    "    \n",
    "    # now, tell the optimizer to update our weights:\n",
    "    sgd_opt.step()\n",
    "\n",
    "    epoch_acc = eval_acc(y_hat, iris_y_train)\n",
    "    \n",
    "    # print out how we're doing:\n",
    "    if i % 100 == 0:\n",
    "        print(f\"epoch {i}, training loss: {loss}, train acc: {eval_acc(y_hat, iris_y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had a more complex model, we could encapsulate it and protect the rest of our code from knowing about it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, output_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.h1 = nn.Linear(input_dims, hidden_dims)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.h2 = nn.Linear(hidden_dims, output_classes)\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.h1(x)\n",
    "        out = self.activation(out) # transform to representation space\n",
    "        out = self.h2(out) # perform classification on representation\n",
    "        return F.log_softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Module`s can contain other `Module`s, and so on. Note that we use it _exactly_ as before. Through the magic of class inheritance, PyTorch knows how to take the gradient of all of the individual pieces in our model, and it all Just Works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, training loss: 1.2584830647253857, train acc: 0.3392857015132904\n",
      "epoch 100, training loss: 0.9672437720063664, train acc: 0.6696428656578064\n",
      "epoch 200, training loss: 0.7861567372903647, train acc: 0.625\n",
      "epoch 300, training loss: 0.6589137083786334, train acc: 0.7678571343421936\n",
      "epoch 400, training loss: 0.5820176319301217, train acc: 0.8035714030265808\n",
      "epoch 500, training loss: 0.5207996661037981, train acc: 0.9017857313156128\n",
      "epoch 600, training loss: 0.48482900852156346, train acc: 0.9375\n",
      "epoch 700, training loss: 0.4564611892110889, train acc: 0.9464285969734192\n",
      "epoch 800, training loss: 0.43010366494617597, train acc: 0.9642857313156128\n",
      "epoch 900, training loss: 0.4042284604935796, train acc: 0.9642857313156128\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP(4, 6, 3).double() # six hidden dims?\n",
    "\n",
    "sgd_opt = torch.optim.SGD(mlp.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 1000\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    sgd_opt.zero_grad() # erase any gradients our optimizer might remember\n",
    "    \n",
    "    # do a forward pass through our data:\n",
    "    y_hat = mlp(iris_x_train) # note that we don't need to call softmax here anymore!\n",
    "    \n",
    "    loss = loss_func(y_hat, iris_y_train)\n",
    "    \n",
    "    # send back gradients:\n",
    "    loss.backward()\n",
    "    \n",
    "    # now, tell the optimizer to update our weights:\n",
    "    sgd_opt.step()\n",
    "\n",
    "    epoch_acc = eval_acc(y_hat, iris_y_train)\n",
    "    \n",
    "    # print out how we're doing:\n",
    "    if i % 100 == 0:\n",
    "        print(f\"epoch {i}, training loss: {loss}, train acc: {eval_acc(y_hat, iris_y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Text!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try training a simple LSTM to do language identification. Since the point of this lab is about PyTorch and not data wrangling, I've encaspulated some code to download and process the data, and put it in a separate file; in the assignment, you will write your own versions of this code. Since you don't have that file, this part of the notebook cannot be run as-is, and so I have left the output in here as an illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'merged.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-21767c3224ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"merged.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'merged.csv' does not exist"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"merged.csv\")\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from util import prep_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2i, i2c = prep_data.build_char_vocab(df)\n",
    "l2i, i2l = prep_data.build_label_vocab(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_sentence = df.head().iloc[0].sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  6,  7,  6,  7,  4,  8,  9, 10,  6,  9,  5,\n",
       "         6,  4, 11,  9,  4, 12,  6, 13, 14,  4, 12,  6,  2,  4, 15, 16, 11, 12,\n",
       "         4, 12,  6,  2,  4, 12,  9, 10,  6, 15,  9,  2, 17, 18])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_data.sentence2tensor(some_sentence, c2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 0,  1,  2,  3,  4,  5,  6,  6,  7,  6,  7,  4,  8,  9, 10,  6,  9,  5,\n",
       "          6,  4, 11,  9,  4, 12,  6, 13, 14,  4, 12,  6,  2,  4, 15, 16, 11, 12,\n",
       "          4, 12,  6,  2,  4, 12,  9, 10,  6, 15,  9,  2, 17, 18])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[prep_data.sentence2tensor(s, c2i) for s in df[:1].sentence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll set up a simple model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(prep_data)\n",
    "li = prep_data.LangID(len(c2i), 10, 20, len(l2i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, query it on our sample sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = li(prep_data.sentence2tensor(some_sentence, c2i).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these are log-probabilities; let's exponentiate to see real-space probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5639, 0.4361], grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have not trained our model, the results here should be pretty random. Let's train using the [ADAM optimizer](https://arxiv.org/abs/1412.6980) (Kingma & Ba, 2015). The process is virtually identical to what we did for our logistic regression model above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per-item loss: 0.007963231764733791, 0/20000\n",
      "per-item loss: 0.6847856640815735, 1000/20000\n",
      "per-item loss: 0.6775606274604797, 2000/20000\n",
      "per-item loss: 0.6763826012611389, 3000/20000\n",
      "per-item loss: 0.6502791047096252, 4000/20000\n",
      "per-item loss: 0.6426841616630554, 5000/20000\n",
      "per-item loss: 0.6270484924316406, 6000/20000\n",
      "per-item loss: 0.6044856905937195, 7000/20000\n",
      "per-item loss: 0.5271546840667725, 8000/20000\n",
      "per-item loss: 0.48755383491516113, 9000/20000\n",
      "per-item loss: 0.39829960465431213, 10000/20000\n",
      "per-item loss: 0.39431822299957275, 11000/20000\n",
      "per-item loss: 0.39963480830192566, 12000/20000\n",
      "per-item loss: 0.31801536679267883, 13000/20000\n",
      "per-item loss: 0.280678927898407, 14000/20000\n",
      "per-item loss: 0.240852952003479, 15000/20000\n",
      "per-item loss: 0.20132911205291748, 16000/20000\n",
      "per-item loss: 0.26558351516723633, 17000/20000\n",
      "per-item loss: 0.28937503695487976, 18000/20000\n",
      "per-item loss: 0.22459350526332855, 19000/20000\n"
     ]
    }
   ],
   "source": [
    "adam_opt = torch.optim.Adam(li.parameters())\n",
    "\n",
    "num_epochs = 1\n",
    "batch_size = 100\n",
    "\n",
    "loss_func = torch.nn.NLLLoss() # since our model gives negative log probs on the output side\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    training_data_x = df.sentence.values\n",
    "    training_data_y = df.lang.values\n",
    "    \n",
    "    pairs = list(zip(training_data_x, training_data_y))\n",
    "    random.shuffle(pairs)\n",
    "    \n",
    "\n",
    "    adam_opt.zero_grad() # erase any gradients our optimizer might remember\n",
    "    loss = 0\n",
    "    \n",
    "    for x_idx, (x, y) in enumerate(pairs):\n",
    "        \n",
    "        if x_idx % batch_size == 0 and x_idx > 0:\n",
    "            adam_opt.zero_grad()\n",
    "            \n",
    "        \n",
    "        x_tens = prep_data.sentence2tensor(x, c2i).unsqueeze(0)\n",
    "        \n",
    "        y_hat = li(x_tens)\n",
    "        \n",
    "        y_tens = torch.tensor(l2i[y])\n",
    "        \n",
    "        loss += loss_func(y_hat.unsqueeze(0), y_tens.unsqueeze(0))\n",
    "    \n",
    "        if x_idx % 1000 == 0:\n",
    "            print(f\"per-item loss: {loss.float() / batch_size}, {x_idx}/{len(pairs)}\")\n",
    "    \n",
    "        # send back gradients:\n",
    "        if x_idx % batch_size == 0 and x_idx > 0:\n",
    "            loss.backward()\n",
    "            # now, tell the optimizer to update our weights:\n",
    "            adam_opt.step()\n",
    "            loss = 0\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it out on an English sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0690, -2.7084])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    lang_probs = li(prep_data.sentence2tensor(\"hello this is an English sentence\", c2i).unsqueeze(0))\n",
    "    print(lang_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eng'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i2l[lang_probs.argmax().item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And on a Spanish one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.4572, -0.2651], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_probs = li(prep_data.sentence2tensor(\"¡Hola! Español es un idioma muy interesante\", c2i).unsqueeze(0))\n",
    "lang_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spa'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i2l[lang_probs.argmax().item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the implementation I've demonstrated here, I'm initializing the RNN's hidden state with random noise. This can help speed up training and improve accuracy, but it does mean that inference is not determinstic- meaning that the same input may give slightly different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
